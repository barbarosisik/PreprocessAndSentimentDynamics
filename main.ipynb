{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk1M5uWM1vVE",
        "outputId": "733abf43-1703-44dd-9998-2d06a95ab043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.10.0-py2.py3-none-any.whl (457 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.9/457.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.10.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install emoji\n",
        "!pip install re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws3fukUf1qMk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uA_5TKO20vp"
      },
      "outputs": [],
      "source": [
        "# Sample code to load and merge datasets - replace with actual dataset loading code\n",
        "def load_and_merge_datasets(file_paths):\n",
        "    merged_data = pd.DataFrame(columns=['TweetID', 'Sentiment', 'Tweet'])\n",
        "    for file_path in file_paths:\n",
        "        data = pd.read_csv(file_path, sep='\\t', header=None, names=['TweetID', 'Sentiment', 'Tweet'])\n",
        "        merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
        "    return merged_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaXHiTMF257d"
      },
      "outputs": [],
      "source": [
        "# Paths to your datasets\n",
        "train_files = ['twitter-2013train-A.tsv', 'twitter-2015train-A.tsv', 'twitter-2016train-A.tsv']\n",
        "test_files = ['twitter-2013test-A.tsv', 'twitter-2014test-A.tsv', 'twitter-2015test-A.tsv', 'twitter-2016test-A.tsv']\n",
        "dev_files = ['twitter-2013dev-A.tsv', 'twitter-2016dev-A.tsv', 'twitter-2016devtest-A.tsv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "Px9KpP7L27lY",
        "outputId": "a53bd291-26a7-4cc6-eae2-1739ced7f9a5"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'twitter-2013train-A.tsv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-65121c85dd28>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and merge datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_merge_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_merge_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_merge_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcombined_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-02e6f393fb00>\u001b[0m in \u001b[0;36mload_and_merge_datasets\u001b[0;34m(file_paths)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TweetID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TweetID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmerged_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter-2013train-A.tsv'"
          ]
        }
      ],
      "source": [
        "# Load and merge datasets\n",
        "train_data = load_and_merge_datasets(train_files)\n",
        "test_data = load_and_merge_datasets(test_files)\n",
        "dev_data = load_and_merge_datasets(dev_files)\n",
        "combined_data = pd.concat([train_data, test_data, dev_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qABgRuWN3T8r"
      },
      "outputs": [],
      "source": [
        "print(combined_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95gSnKdak8Wf"
      },
      "source": [
        "## Visualization of the Sentiment Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMxryL28iw3M"
      },
      "outputs": [],
      "source": [
        "# Ensure that the 'Sentiment' column is treated as a string and strip any whitespace\n",
        "combined_data['Sentiment'] = combined_data['Sentiment'].astype(str).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXIjuk6VCau2"
      },
      "outputs": [],
      "source": [
        "# Map sentiment labels to a consistent format if they're not already\n",
        "sentiment_mapping = {'positive': 'positive', 'negative': 'negative', 'neutral': 'neutral'}\n",
        "combined_data['Sentiment'] = combined_data['Sentiment'].map(sentiment_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-GMwJTZB4H-"
      },
      "source": [
        "Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lyosc_YD6T2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy2uLj68HxJO"
      },
      "outputs": [],
      "source": [
        "positive_tweets = preprocess_data(data=train_data_non_nan, use_stemming=True, use_lemmatization=True, remove_emojis=True,\n",
        "                    remove_urls_and_html_tags=True, remove_punctuation_and_special_chars=True,\n",
        "                    remove_stopwords=True)[preprocess_data(data=train_data_non_nan, use_stemming=True,\n",
        "                                                           use_lemmatization=True,\n",
        "                                                           remove_emojis=True,\n",
        "                                                           remove_urls_and_html_tags=True,\n",
        "                                                           remove_punctuation_and_special_chars=True,\n",
        "                                                           remove_stopwords=True)[\n",
        "                                               'Sentiment'] == 'positive']['Tweet']\n",
        "negative_tweets = preprocess_data(data=train_data_non_nan, use_stemming=True, use_lemmatization=True, remove_emojis=True,\n",
        "                    remove_urls_and_html_tags=True, remove_punctuation_and_special_chars=True,\n",
        "                    remove_stopwords=True)[preprocess_data(data=train_data_non_nan, use_stemming=True,\n",
        "                                                           use_lemmatization=True,\n",
        "                                                           remove_emojis=True,\n",
        "                                                           remove_urls_and_html_tags=True,\n",
        "                                                           remove_punctuation_and_special_chars=True,\n",
        "                                                           remove_stopwords=True)\n",
        "                                           ['Sentiment'] == 'negative']['Tweet']\n",
        "neutral_tweets = preprocess_data(data=train_data_non_nan, use_stemming=True, use_lemmatization=True, remove_emojis=True,\n",
        "                                 remove_urls_and_html_tags=True, remove_punctuation_and_special_chars=True,\n",
        "                                 remove_stopwords=True)[preprocess_data(data=train_data_non_nan, use_stemming=True,\n",
        "                                                                        use_lemmatization=True,\n",
        "                                                                        remove_emojis=True,\n",
        "                                                                        remove_urls_and_html_tags=True,\n",
        "                                                                        remove_punctuation_and_special_chars=True,\n",
        "                                                                        remove_stopwords=True)\n",
        "                                                        ['Sentiment'] == 'neutral'][\"Tweet\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exyBrN21Dl4R"
      },
      "outputs": [],
      "source": [
        "# Generate Word Clouds\n",
        "def generate_word_cloud(tweets, title):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    wc = WordCloud(background_color='white', max_words=500, width=800, height=400, collocations=False).generate(\" \".join(tweets.astype(str)))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fu30vy9EMeG"
      },
      "outputs": [],
      "source": [
        "generate_word_cloud(positive_tweets, 'Word Cloud for Positive Tweets')\n",
        "generate_word_cloud(negative_tweets, 'Word Cloud for Negative Tweets')\n",
        "generate_word_cloud(neutral_tweets, 'Word Cloud for Neutral Tweets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou27e8tucQkn"
      },
      "source": [
        "**Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKet0zvgdY5r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK9BJ8vNcznV"
      },
      "outputs": [],
      "source": [
        "# Drop rows with NaN values\n",
        "combined_data = combined_data.dropna(subset=['Tweet', 'Sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-1yzgOJdG8i"
      },
      "outputs": [],
      "source": [
        "# Split the combined dataset into features (tweets) and labels (sentiments)\n",
        "X = combined_data['Tweet']\n",
        "y = combined_data['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKDEbjC2dJmr"
      },
      "outputs": [],
      "source": [
        "# Vectorize the features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_vectorized = vectorizer.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU6_xe80dKDL"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAf1XBjHdOPy"
      },
      "outputs": [],
      "source": [
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(class_weight='balanced',C=0.8,max_iter=1000)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxPSJulTdQZF"
      },
      "outputs": [],
      "source": [
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSVPexZ_dR_F"
      },
      "outputs": [],
      "source": [
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xSinsO5dbTE"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(sentiment_mapping.keys())\n",
        "ax.yaxis.set_ticklabels(sentiment_mapping.keys())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7jnyjBjIEf2"
      },
      "source": [
        "Line Chart for Evaluation of the Sentiment Distribution for Years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayC6sBmXILgf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puP6jeWlIOsF"
      },
      "outputs": [],
      "source": [
        "# File paths for the datasets of each year\n",
        "file_paths = {\n",
        "    '2013': ['twitter-2013train-A.tsv', 'twitter-2013dev-A.tsv', 'twitter-2013test-A.tsv'],\n",
        "    '2014': ['twitter-2014sarcasm-A.tsv', 'twitter-2014test-A.tsv'],\n",
        "    '2015': ['twitter-2015test-A.tsv', 'twitter-2015train-A.tsv'],\n",
        "    '2016': ['twitter-2016dev-A.tsv', 'twitter-2016devtest-A.tsv', 'twitter-2016test-A.tsv', 'twitter-2016train-A.tsv']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElgxqAVJITYq"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to hold sentiment counts\n",
        "sentiment_counts_by_year = {year: {'positive': 0, 'negative': 0, 'neutral': 0} for year in file_paths}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9-Vu9-xIydK"
      },
      "outputs": [],
      "source": [
        "# Load the datasets and count sentiments for each year\n",
        "for year, paths in file_paths.items():\n",
        "    for path in paths:\n",
        "        # Read the dataset\n",
        "        data = pd.read_csv(f'{path}', sep='\\t', header=None, names=['TweetID', 'Sentiment', 'Tweet'])\n",
        "\n",
        "        # Drop rows with NaN values\n",
        "        data = data.dropna(subset=['Tweet', 'Sentiment'])\n",
        "\n",
        "        # Count the sentiments\n",
        "        sentiment_counts = data['Sentiment'].value_counts()\n",
        "\n",
        "        # Update the sentiment counts for the year\n",
        "        for sentiment in ['positive', 'negative', 'neutral']:\n",
        "            if sentiment in sentiment_counts:\n",
        "                sentiment_counts_by_year[year][sentiment] += sentiment_counts[sentiment]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMdHUwbHI0jB"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for the plot\n",
        "years = sorted(sentiment_counts_by_year.keys())\n",
        "positives = [sentiment_counts_by_year[year]['positive'] for year in years]\n",
        "negatives = [sentiment_counts_by_year[year]['negative'] for year in years]\n",
        "neutrals = [sentiment_counts_by_year[year]['neutral'] for year in years]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmOMtC2OI2Rj"
      },
      "outputs": [],
      "source": [
        "# Create the line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(years, positives, 'g-o', label='Positive')  # Green line with dots for positives\n",
        "plt.plot(years, negatives, 'r-o', label='Negative')  # Red line with dots for negatives\n",
        "plt.plot(years, neutrals, 'b-o', label='Neutral')  # Blue line with dots for neutrals\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpodte-7cWT2"
      },
      "source": [
        "**Distribution of Sentiments (Bar Plot)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3oF8CGbiVUn"
      },
      "outputs": [],
      "source": [
        "# Calculate the sentiment counts\n",
        "sentiment_counts = combined_data['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvat63eNCcfg"
      },
      "outputs": [],
      "source": [
        "# Plot the sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sentiment_counts.plot(kind='bar', color=['green', 'red', 'blue'])\n",
        "plt.title('Sentiment Distribution in the Combined Dataset')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhe2Q67clHdn"
      },
      "outputs": [],
      "source": [
        "# Map sentiments to numerical values\n",
        "sentiment_mapping = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
        "train_data['Sentiment'] = train_data['Sentiment'].map(sentiment_mapping)\n",
        "test_data['Sentiment'] = test_data['Sentiment'].map(sentiment_mapping)\n",
        "dev_data['Sentiment'] = dev_data['Sentiment'].map(sentiment_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVYmBTRBusw"
      },
      "source": [
        "Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-_QNZm3lLHB"
      },
      "outputs": [],
      "source": [
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_data['Tweet'].fillna(''))  # Fill NaN with empty strings\n",
        "y_train = train_data['Sentiment']\n",
        "X_test = vectorizer.transform(test_data['Tweet'].fillna(''))  # Also fill NaN with empty strings in the test set\n",
        "y_test = test_data['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sFgCzWImpQ8"
      },
      "outputs": [],
      "source": [
        "# Training a Logistic Regression model as a baseline\n",
        "model = LogisticRegression(class_weight='balanced',C=0.8 ,max_iter=1000)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUQW6R82n3YG"
      },
      "source": [
        "Removing the NaN value rows, because not allowed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrg1J4g5n2va"
      },
      "outputs": [],
      "source": [
        "# Find rows where y_test_no_stop is not NaN\n",
        "valid_indices = y_test.notna()\n",
        "\n",
        "# Filter both X_test_no_stop and y_test_no_stop to remove NaNs\n",
        "X_test_filtered = X_test[valid_indices]\n",
        "y_test_filtered = y_test[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Uw5vZStFtCL"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model\n",
        "y_pred = model.predict(X_test_filtered)\n",
        "report = classification_report(y_test_filtered, y_pred, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro = precision_score(y_test_filtered, y_pred, average='macro')\n",
        "recall_macro = recall_score(y_test_filtered, y_pred, average='macro')\n",
        "f1_macro = f1_score(y_test_filtered, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test_filtered, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr8CwCePF06s"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report for Baseline Model:\\n\", report)\n",
        "print(\"Macro-average Precision:\", precision_macro)\n",
        "print(\"Macro-average Recall:\", recall_macro)\n",
        "print(\"Macro-average F1-score:\", f1_macro)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uet1VzD1qtKQ"
      },
      "source": [
        "# Implementing Preprocessing Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3OwNtZ5FJPT"
      },
      "source": [
        "# Removing Punctuation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxwtRdM_FOt2"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation_and_special_characters(text):\n",
        "    # Remove punctuation and special characters\n",
        "    text_without_punctuations = ''.join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    return text_without_punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afaat1L5EClC"
      },
      "source": [
        "# Removing URLs and HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40lhqya1EJJe"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ3wizfXEBSf"
      },
      "outputs": [],
      "source": [
        "def remove_urls_and_html(text):\n",
        "    # Remove URLs\n",
        "    text_without_urls = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text_without_html = re.sub('<.*?>', '', text_without_urls)\n",
        "\n",
        "    return text_without_html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUsMR-0Jq0Uf"
      },
      "source": [
        "# Stemming/Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU-1qaDatwpk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6GmjnuYt1tY"
      },
      "outputs": [],
      "source": [
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_and_lemmatize_text(text):\n",
        "    # First apply stemming\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    # Then apply lemmatization\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in stemmed_text.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2kEgBcwvWOb"
      },
      "outputs": [],
      "source": [
        "# To not to change the raw dataset to use as it is on the next pre-processing methods,\n",
        "# we wanted to create a temporary datasets that lemmatized and stemmed\n",
        "train_data_stem_lem = train_data.copy()\n",
        "train_data_stem_lem['Tweet'] = train_data_stem_lem['Tweet'].apply(stem_and_lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP-19cd2zzrg"
      },
      "outputs": [],
      "source": [
        "# Fill NaN values with empty strings in both datasets\n",
        "train_data_stem_lem['Tweet'] = train_data_stem_lem['Tweet'].fillna('')\n",
        "test_data['Tweet'] = test_data['Tweet'].fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jky24IPQyMqL"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_stem_lem = TfidfVectorizer(max_features=5000)\n",
        "X_train_stem_lem = vectorizer_stem_lem.fit_transform(train_data_stem_lem['Tweet'])\n",
        "y_train_stem_lem = train_data_stem_lem['Sentiment']\n",
        "X_test_stem_lem = vectorizer_stem_lem.transform(test_data['Tweet'])  # Assuming test_data is already preprocessed\n",
        "y_test_stem_lem = test_data['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZbb7qk8yibE"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "model_stem_lem = LogisticRegression(class_weight='balanced',C=0.8,max_iter=1000)\n",
        "model_stem_lem.fit(X_train_stem_lem, y_train_stem_lem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTf1NUEe5oxl"
      },
      "outputs": [],
      "source": [
        "# Filtering out rows with NaN values in y_test_stem_lem\n",
        "valid_indices = y_test_stem_lem.notna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZT1M6XI0lr0"
      },
      "outputs": [],
      "source": [
        "# Ensuring that X_test_stem_lem and y_test_stem_lem have the same rows\n",
        "X_test_stem_lem_filtered = X_test_stem_lem[valid_indices]\n",
        "y_test_stem_lem_filtered = y_test_stem_lem[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14UU1RqVj98O"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_stem_lem = model_stem_lem.predict(X_test_stem_lem_filtered)\n",
        "report_stem_lem = classification_report(y_test_stem_lem_filtered, y_pred_stem_lem, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_stem_lem = precision_score(y_test_stem_lem_filtered, y_pred_stem_lem, average='macro')\n",
        "recall_macro_stem_lem = recall_score(y_test_stem_lem_filtered, y_pred_stem_lem, average='macro')\n",
        "f1_macro_stem_lem = f1_score(y_test_stem_lem_filtered, y_pred_stem_lem, average='macro')\n",
        "accuracy_stem_lem = accuracy_score(y_test_stem_lem_filtered, y_pred_stem_lem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BhlFpdRzDHo"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report for Model with Stemming and Lematization:\\n\", report_stem_lem)\n",
        "print(\"macro-average Precision:\", precision_macro_stem_lem)\n",
        "print(\"macro-average Recall:\", recall_macro_stem_lem)\n",
        "print(\"macro-average F1-score:\", f1_macro_stem_lem)\n",
        "print(\"Accuracy:\", accuracy_stem_lem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKJOjPwp0-Th"
      },
      "source": [
        "As we can tell, stemming and lemmatization for a dataset like this, decreases the results taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vfujfLN1cTl"
      },
      "source": [
        "# Stop Words Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPmVNmwt4guV"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2kD5xXY26gO"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEHZEvcY4kd-"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0NbCeYF4o6V"
      },
      "outputs": [],
      "source": [
        "# Create new temporary datasets with stop words removed\n",
        "train_data_no_stop = train_data.copy()\n",
        "train_data_no_stop['Tweet'] = train_data_no_stop['Tweet'].apply(remove_stopwords)\n",
        "\n",
        "test_data_no_stop = test_data.copy()\n",
        "test_data_no_stop['Tweet'] = test_data_no_stop['Tweet'].apply(remove_stopwords)\n",
        "\n",
        "dev_data_no_stop = dev_data.copy()\n",
        "dev_data_no_stop['Tweet'] = dev_data_no_stop['Tweet'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSHeFX674zkM"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_no_stop = TfidfVectorizer(max_features=5000)\n",
        "X_train_no_stop = vectorizer_no_stop.fit_transform(train_data_no_stop['Tweet'])\n",
        "y_train_no_stop = train_data_no_stop['Sentiment']\n",
        "X_test_no_stop = vectorizer_no_stop.transform(test_data_no_stop['Tweet'])\n",
        "y_test_no_stop = test_data_no_stop['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExsSTOpv41Mo"
      },
      "outputs": [],
      "source": [
        "# Training the Logistic Regression model\n",
        "model_no_stop = LogisticRegression(class_weight='balanced',C=0.8,max_iter=1000)\n",
        "model_no_stop.fit(X_train_no_stop, y_train_no_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC2VsJiP7TUh"
      },
      "outputs": [],
      "source": [
        "# Find rows where y_test_no_stop is not NaN\n",
        "valid_indices = y_test_no_stop.notna()\n",
        "\n",
        "# Filter both X_test_no_stop and y_test_no_stop to remove NaNs\n",
        "X_test_no_stop_filtered = X_test_no_stop[valid_indices]\n",
        "y_test_no_stop_filtered = y_test_no_stop[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MJSZgZ56r9n"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_no_stop = model_no_stop.predict(X_test_no_stop_filtered)\n",
        "report_no_stop = classification_report(y_test_no_stop_filtered, y_pred_no_stop, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_no_stop = precision_score(y_test_no_stop_filtered, y_pred_no_stop, average='macro')\n",
        "recall_macro_no_stop = recall_score(y_test_no_stop_filtered, y_pred_no_stop, average='macro')\n",
        "f1_macro_no_stop = f1_score(y_test_no_stop_filtered, y_pred_no_stop, average='macro')\n",
        "accuracy_no_stop = accuracy_score(y_test_no_stop_filtered, y_pred_no_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOkDptb0jgvf"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report for Model with Stop Words Removed:\\n\", report_no_stop)\n",
        "print(\"macro-average Precision:\", precision_macro_no_stop)\n",
        "print(\"macro-average Recall:\", recall_macro_no_stop)\n",
        "print(\"macro-average F1-score:\", f1_macro_no_stop)\n",
        "print(\"Accuracy:\", accuracy_no_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puDxXCaeblMk"
      },
      "source": [
        "# Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh68DbZzboNU"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9NihtRken7q"
      },
      "outputs": [],
      "source": [
        "# Copying the raw datasets\n",
        "train_data = train_data.copy()\n",
        "test_data = test_data.copy()\n",
        "dev_data = dev_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYfdViQcevFI"
      },
      "outputs": [],
      "source": [
        "# Applying lowercasing to the copied datasets\n",
        "train_data_lower = train_data.copy()\n",
        "train_data_lower['Tweet'] = train_data_lower['Tweet'].str.lower()\n",
        "\n",
        "test_data_lower = test_data.copy()\n",
        "test_data_lower['Tweet'] = test_data_lower['Tweet'].str.lower()\n",
        "\n",
        "dev_data_lower = dev_data.copy()\n",
        "dev_data_lower['Tweet'] = dev_data_lower['Tweet'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzWbhst4dXH5"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_lower = TfidfVectorizer(max_features=5000)\n",
        "X_train_lower = vectorizer_lower.fit_transform(train_data_lower['Tweet'])\n",
        "y_train_lower = train_data_lower['Sentiment']\n",
        "X_test_lower = vectorizer_lower.transform(test_data_lower['Tweet'])\n",
        "y_test_lower = test_data_lower['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pvqHx04dZZx"
      },
      "outputs": [],
      "source": [
        "# Training the SVM model (others were with Logistic Regression)\n",
        "model_lower = SVC(kernel='linear')  # Using a linear kernel\n",
        "model_lower.fit(X_train_lower, y_train_lower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FJtowNHhIkn"
      },
      "outputs": [],
      "source": [
        "# Find rows where y_test_no_stop is not NaN\n",
        "valid_indices = y_test_lower.notna()\n",
        "\n",
        "# Filter both X_test_no_stop and y_test_no_stop to remove NaNs\n",
        "X_test_lower_filtered = X_test_lower[valid_indices]\n",
        "y_test_lower_filtered = y_test_lower[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAe3LjmudeWM"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_lower = model_lower.predict(X_test_lower_filtered)\n",
        "report_lower = classification_report(y_test_lower_filtered, y_pred_lower, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_lower = precision_score(y_test_lower_filtered, y_pred_lower, average='macro')\n",
        "recall_macro_lower = recall_score(y_test_lower_filtered, y_pred_lower, average='macro')\n",
        "f1_macro_lower = f1_score(y_test_lower_filtered, y_pred_lower, average='macro')\n",
        "accuracy_lower = accuracy_score(y_test_lower_filtered, y_pred_lower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFn11fogdfIs"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Lowercased Model:\\n\", report_lower)\n",
        "print(\"macro-average Precision:\", precision_macro_lower)\n",
        "print(\"macro-average Recall:\", recall_macro_lower)\n",
        "print(\"macro-average F1-score:\", f1_macro_lower)\n",
        "print(\"Accuracy:\", accuracy_lower)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQcSTfF0DJMn"
      },
      "source": [
        "**Training The Model with Baseline (Logistic Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bmL-4mrRN60"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_lower = TfidfVectorizer(max_features=5000)\n",
        "X_train_lower = vectorizer_lower.fit_transform(train_data_lower['Tweet'])\n",
        "y_train_lower = train_data_lower['Sentiment']\n",
        "X_test_lower = vectorizer_lower.transform(test_data_lower['Tweet'])\n",
        "y_test_lower = test_data_lower['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB-HnmfYKBUK"
      },
      "outputs": [],
      "source": [
        "# Training the Logistic Regression model\n",
        "model_lower_lr = LogisticRegression(class_weight='balanced',C=0.8,max_iter=1000)\n",
        "model_lower_lr.fit(X_train_lower, y_train_lower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU4UDmPvQ6fG"
      },
      "outputs": [],
      "source": [
        "# Filter out NaN values and align X and y\n",
        "valid_indices = y_test_lower.notna()\n",
        "X_test_lower_filtered = X_test_lower[valid_indices]\n",
        "y_test_lower_filtered = y_test_lower[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82mr3qI7PJbE"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_lower_lr = model_lower_lr.predict(X_test_lower_filtered)\n",
        "report_lower_lr = classification_report(y_test_lower_filtered, y_pred_lower_lr, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_lower_lr = precision_score(y_test_lower_filtered, y_pred_lower_lr, average='macro')\n",
        "recall_macro_lower_lr = recall_score(y_test_lower_filtered, y_pred_lower_lr, average='macro')\n",
        "f1_macro_lower_lr = f1_score(y_test_lower_filtered, y_pred_lower_lr, average='macro')\n",
        "accuracy_lower_lr = accuracy_score(y_test_lower_filtered, y_pred_lower_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo66Qjn9PN_8"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Lowercased Model with Logistic Regression:\\n\", report_lower_lr)\n",
        "print(\"Macro-average Precision:\", precision_macro_lower_lr)\n",
        "print(\"Macro-average Recall:\", recall_macro_lower_lr)\n",
        "print(\"Macro-average F1-score:\", f1_macro_lower_lr)\n",
        "print(\"Accuracy:\", accuracy_lower_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFmVrDPTlT7I"
      },
      "source": [
        "# Removing Punctuation and Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpHVCuj4qtVI"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keiC7WbKqwpU"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMKsySVPlWum"
      },
      "outputs": [],
      "source": [
        "# Copying the raw datasets\n",
        "train_data = train_data.copy()\n",
        "test_data = test_data.copy()\n",
        "dev_data = dev_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ4hpGHUqyDV"
      },
      "outputs": [],
      "source": [
        "# Apply the function to remove punctuation and special characters\n",
        "train_data_no_punct = train_data.copy()\n",
        "train_data_no_punct['Tweet'] = train_data_no_punct['Tweet'].apply(remove_punctuation)\n",
        "\n",
        "test_data_no_punct = test_data.copy()\n",
        "test_data_no_punct['Tweet'] = test_data_no_punct['Tweet'].apply(remove_punctuation)\n",
        "\n",
        "dev_data_no_punct = dev_data.copy()\n",
        "dev_data_no_punct['Tweet'] = dev_data_no_punct['Tweet'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdLl1NFhrbKl"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_no_punct = TfidfVectorizer(max_features=5000)\n",
        "X_train_no_punct = vectorizer_no_punct.fit_transform(train_data_no_punct['Tweet'])\n",
        "y_train_no_punct = train_data_no_punct['Sentiment']\n",
        "X_test_no_punct = vectorizer_no_punct.transform(test_data_no_punct['Tweet'])\n",
        "y_test_no_punct = test_data_no_punct['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_WQofjOreKs"
      },
      "outputs": [],
      "source": [
        "# Training the Random Forest model\n",
        "model_no_punct = RandomForestClassifier()\n",
        "model_no_punct.fit(X_train_no_punct, y_train_no_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLNxqqLCtm-2"
      },
      "outputs": [],
      "source": [
        "# Find rows where y_test_no_stop is not NaN\n",
        "valid_indices = y_test_no_punct.notna()\n",
        "\n",
        "# Filter both X_test_no_stop and y_test_no_stop to remove NaNs\n",
        "X_test_no_punct_filtered = X_test_no_punct[valid_indices]\n",
        "y_test_no_punct_filtered = y_test_no_punct[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecLZWHkEsFnk"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_no_punct = model_no_punct.predict(X_test_no_punct_filtered)\n",
        "report_no_punct = classification_report(y_test_no_punct_filtered, y_pred_no_punct, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_no_punct = precision_score(y_test_no_punct_filtered, y_pred_no_punct, average='macro')\n",
        "recall_macro_no_punct = recall_score(y_test_no_punct_filtered, y_pred_no_punct, average='macro')\n",
        "f1_macro_no_punct = f1_score(y_test_no_punct_filtered, y_pred_no_punct, average='macro')\n",
        "accuracy_no_punct = accuracy_score(y_test_no_punct_filtered, y_pred_no_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6Axko6WuEmK"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Punctuation Model:\\n\", report_no_punct)\n",
        "print(\"macro-average Precision:\", precision_macro_no_punct)\n",
        "print(\"macro-average Recall:\", recall_macro_no_punct)\n",
        "print(\"macro-average F1-score:\", f1_macro_no_punct)\n",
        "print(\"Accuracy:\", accuracy_no_punct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkdmT65NDZpu"
      },
      "source": [
        "**Training The Model with Baseline (Logistic Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQaqnLTD4Pw"
      },
      "outputs": [],
      "source": [
        "# Training the Logistic Regression model\n",
        "model_no_punct_lr = LogisticRegression(class_weight='balanced',C=0.8,max_iter=1000)\n",
        "model_no_punct_lr.fit(X_train_no_punct, y_train_no_punct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9jsRLl8UKae"
      },
      "outputs": [],
      "source": [
        "# Filter out rows where y_test_no_punct is NaN\n",
        "valid_indices = y_test_no_punct.notna()\n",
        "X_test_no_punct_filtered = X_test_no_punct[valid_indices]\n",
        "y_test_no_punct_filtered = y_test_no_punct[valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgLRNi0rToGX"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model with the filtered data\n",
        "y_pred_no_punct_lr = model_no_punct_lr.predict(X_test_no_punct_filtered)\n",
        "report_no_punct_lr = classification_report(y_test_no_punct_filtered, y_pred_no_punct_lr, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_no_punct_lr = precision_score(y_test_no_punct_filtered, y_pred_no_punct_lr, average='macro')\n",
        "recall_macro_no_punct_lr = recall_score(y_test_no_punct_filtered, y_pred_no_punct_lr, average='macro')\n",
        "f1_macro_no_punct_lr = f1_score(y_test_no_punct_filtered, y_pred_no_punct_lr, average='macro')\n",
        "accuracy_no_punct_lr = accuracy_score(y_test_no_punct_filtered, y_pred_no_punct_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGqiXOBiTosN"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Logistic Regression Model with Punctuation and Special Characters Removed:\\n\", report_no_punct_lr)\n",
        "print(\"Macro-average Precision:\", precision_macro_no_punct_lr)\n",
        "print(\"Macro-average Recall:\", recall_macro_no_punct_lr)\n",
        "print(\"Macro-average F1-score:\", f1_macro_no_punct_lr)\n",
        "print(\"Accuracy:\", accuracy_no_punct_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ior45aWalXoI"
      },
      "source": [
        "# Removing URLs and HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xjtHIE3la_6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL1WKo3wwDvt"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWdVqnRYwzMi"
      },
      "outputs": [],
      "source": [
        "# Copying the raw datasets\n",
        "train_data_no_url_html = train_data.copy()\n",
        "train_data_no_url_html['Tweet'] = train_data_no_url_html['Tweet'].apply(remove_urls).apply(remove_html_tags)\n",
        "\n",
        "test_data_no_url_html = test_data.copy()\n",
        "test_data_no_url_html['Tweet'] = test_data_no_url_html['Tweet'].apply(remove_urls).apply(remove_html_tags)\n",
        "\n",
        "dev_data_no_url_html = dev_data.copy()\n",
        "dev_data_no_url_html['Tweet'] = dev_data_no_url_html['Tweet'].apply(remove_urls).apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH75O1VzxmZm"
      },
      "outputs": [],
      "source": [
        "# Filter out NaN values\n",
        "train_data_no_url_html = train_data_no_url_html.dropna()\n",
        "test_data_no_url_html = test_data_no_url_html.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj5-sDkKyKLg"
      },
      "outputs": [],
      "source": [
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_data_no_url_html['Tweet'])\n",
        "\n",
        "X_train_nn = tokenizer.texts_to_sequences(train_data_no_url_html['Tweet'])\n",
        "X_train_nn = pad_sequences(X_train_nn, maxlen=100)\n",
        "\n",
        "X_test_nn = tokenizer.texts_to_sequences(test_data_no_url_html['Tweet'])\n",
        "X_test_nn = pad_sequences(X_test_nn, maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqEk0fUvyPWy"
      },
      "outputs": [],
      "source": [
        "# Convert labels to categorical\n",
        "y_train_nn = to_categorical(np.array(train_data_no_url_html['Sentiment']))\n",
        "y_test_nn = to_categorical(np.array(test_data_no_url_html['Sentiment']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgBXnUgmySLX"
      },
      "outputs": [],
      "source": [
        "# Neural Network Model\n",
        "model_nn = Sequential()\n",
        "model_nn.add(Embedding(input_dim=5000, output_dim=64, input_length=100))\n",
        "model_nn.add(GlobalAveragePooling1D())\n",
        "model_nn.add(Dense(3, activation='softmax'))  # Assuming 3 classes: Neutral, Negative, Positive\n",
        "\n",
        "model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_nn.fit(X_train_nn, y_train_nn, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhMXeigWyVuH"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_nn = np.argmax(model_nn.predict(X_test_nn), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc_gncY5yXTr"
      },
      "outputs": [],
      "source": [
        "# Convert y_test from categorical to single label\n",
        "y_test_single_label = np.argmax(y_test_nn, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FriYG3nvzaHl"
      },
      "outputs": [],
      "source": [
        "# Classification report and other metrics\n",
        "report_nn = classification_report(y_test_single_label, y_pred_nn, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_nn = precision_score(y_test_single_label, y_pred_nn, average='macro')\n",
        "recall_macro_nn = recall_score(y_test_single_label, y_pred_nn, average='macro')\n",
        "f1_macro_nn = f1_score(y_test_single_label, y_pred_nn, average='macro')\n",
        "accuracy_nn = accuracy_score(y_test_single_label, y_pred_nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZJEhftMzbt-"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Neural Network Model:\\n\", report_nn)\n",
        "print(\"macro-average Precision:\", precision_macro_nn)\n",
        "print(\"macro-average Recall:\", recall_macro_nn)\n",
        "print(\"macro-average F1-score:\", f1_macro_nn)\n",
        "print(\"Accuracy:\", accuracy_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgeG-iU_D6CL"
      },
      "source": [
        "Training The Model with Baseline (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_RYYzi6WGts"
      },
      "outputs": [],
      "source": [
        "# Filter out NaN values\n",
        "train_data_no_url_html = train_data_no_url_html.dropna()\n",
        "test_data_no_url_html = test_data_no_url_html.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGw7ToYOWJJV"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_no_url_html = TfidfVectorizer(max_features=5000)\n",
        "X_train_no_url_html = vectorizer_no_url_html.fit_transform(train_data_no_url_html['Tweet'])\n",
        "y_train_no_url_html = train_data_no_url_html['Sentiment']\n",
        "X_test_no_url_html = vectorizer_no_url_html.transform(test_data_no_url_html['Tweet'])\n",
        "y_test_no_url_html = test_data_no_url_html['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aulYCbidWJzc"
      },
      "outputs": [],
      "source": [
        "# Training the Logistic Regression model\n",
        "model_no_url_html_lr = LogisticRegression(class_weight='balanced', C=0.8, max_iter=1000)\n",
        "model_no_url_html_lr.fit(X_train_no_url_html, y_train_no_url_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiF5EGfNWSJj"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_no_url_html_lr = model_no_url_html_lr.predict(X_test_no_url_html)\n",
        "report_no_url_html_lr = classification_report(y_test_no_url_html, y_pred_no_url_html_lr, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_no_url_html_lr = precision_score(y_test_no_url_html, y_pred_no_url_html_lr, average='macro')\n",
        "recall_macro_no_url_html_lr = recall_score(y_test_no_url_html, y_pred_no_url_html_lr, average='macro')\n",
        "f1_macro_no_url_html_lr = f1_score(y_test_no_url_html, y_pred_no_url_html_lr, average='macro')\n",
        "accuracy_no_url_html_lr = accuracy_score(y_test_no_url_html, y_pred_no_url_html_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcrN2W6YWTrW"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Logistic Regression Model with URLs and HTML Tags Removed:\\n\", report_no_url_html_lr)\n",
        "print(\"Macro-average Precision:\", precision_macro_no_url_html_lr)\n",
        "print(\"Macro-average Recall:\", recall_macro_no_url_html_lr)\n",
        "print(\"Macro-average F1-score:\", f1_macro_no_url_html_lr)\n",
        "print(\"Accuracy:\", accuracy_no_url_html_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnet6KqJ7N1y"
      },
      "source": [
        "# Removing Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T99WRP_Mlkt3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSkGJkxGBjAi"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzbDdsDIBOk3"
      },
      "outputs": [],
      "source": [
        "# Copying the raw datasets\n",
        "train_data_no_numbers = train_data.copy()\n",
        "train_data_no_numbers['Tweet'] = train_data_no_numbers['Tweet'].apply(remove_numbers)\n",
        "\n",
        "test_data_no_numbers = test_data.copy()\n",
        "test_data_no_numbers['Tweet'] = test_data_no_numbers['Tweet'].apply(remove_numbers)\n",
        "\n",
        "dev_data_no_numbers = dev_data.copy()\n",
        "dev_data_no_numbers['Tweet'] = dev_data_no_numbers['Tweet'].apply(remove_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klY2IjofBQDV"
      },
      "outputs": [],
      "source": [
        "# Filter out NaN values\n",
        "train_data_no_numbers = train_data_no_numbers.dropna()\n",
        "test_data_no_numbers = test_data_no_numbers.dropna()\n",
        "dev_data_no_numbers = dev_data_no_numbers.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgmioHq0_4z9"
      },
      "outputs": [],
      "source": [
        "# Vectorization with TF-IDF\n",
        "vectorizer_no_numbers = TfidfVectorizer(max_features=5000)\n",
        "X_train_no_numbers = vectorizer_no_numbers.fit_transform(train_data_no_numbers['Tweet'])\n",
        "y_train_no_numbers = train_data_no_numbers['Sentiment']\n",
        "X_test_no_numbers = vectorizer_no_numbers.transform(test_data_no_numbers['Tweet'])\n",
        "y_test_no_numbers = test_data_no_numbers['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvGeM9-sABU-"
      },
      "outputs": [],
      "source": [
        "# Training the Logistic Regression model\n",
        "model_no_numbers = LogisticRegression(class_weight='balanced',C=0.8,max_iter=1000)\n",
        "model_no_numbers.fit(X_train_no_numbers, y_train_no_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RltqNh2qAEo2"
      },
      "outputs": [],
      "source": [
        "# Predicting and evaluating the model\n",
        "y_pred_no_numbers = model_no_numbers.predict(X_test_no_numbers)\n",
        "report_no_numbers = classification_report(y_test_no_numbers, y_pred_no_numbers, target_names=['Neutral', 'Negative', 'Positive'])\n",
        "precision_macro_no_numbers = precision_score(y_test_no_numbers, y_pred_no_numbers, average='macro')\n",
        "recall_macro_no_numbers = recall_score(y_test_no_numbers, y_pred_no_numbers, average='macro')\n",
        "f1_macro_no_numbers = f1_score(y_test_no_numbers, y_pred_no_numbers, average='macro')\n",
        "accuracy_no_numbers = accuracy_score(y_test_no_numbers, y_pred_no_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZLRpJJ8AHKK"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"Classification Report for Logistic Regression Model with Numbers Removed:\\n\", report_no_numbers)\n",
        "print(\"macro-average Precision:\", precision_macro_no_numbers)\n",
        "print(\"macro-average Recall:\", recall_macro_no_numbers)\n",
        "print(\"macro-average F1-score:\", f1_macro_no_numbers)\n",
        "print(\"Accuracy:\", accuracy_no_numbers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}